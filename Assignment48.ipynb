{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1b3e96-a0de-4654-b1d3-2f7b33ba3963",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9066c1-3834-4fd8-9768-645e6cf654d1",
   "metadata": {},
   "source": [
    "### R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "### Formula: R^2= 1- rss/tss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220e7ef-709e-43e6-bb8a-b919e8ea5104",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dcb6b-28ff-44c0-995c-48060744100f",
   "metadata": {},
   "source": [
    "### Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected. \n",
    "### The predicted R-squared, unlike the adjusted R-squared, is used to indicate how well a regression model predicts responses for new observations. So where the adjusted R-squared can provide an accurate model that fits the current data, the predicted R-squared determines how likely it is that this model will be accurate for future data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42af11-e843-4162-9bad-24cf4282b212",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffad3a-2492-41db-b0ca-3e3f380cd0eb",
   "metadata": {},
   "source": [
    "### It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41898327-a05f-4243-a352-bd86c4fc774c",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52aeadc-f3de-432d-b3e6-f5c1db44a4f2",
   "metadata": {},
   "source": [
    "### The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "### MSE= sum over i [(Yi-Yi^)^2]/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f91993-fc42-4b54-9d5f-07529d2ee816",
   "metadata": {},
   "source": [
    "### Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
    "### MAE= sum over i |Yi-Yi^| /n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a2477-da38-446a-b872-aa3cd7b5802a",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1bc75-f264-4147-9e50-435c692461fd",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185809c7-2275-4251-ade0-38c834442b2d",
   "metadata": {},
   "source": [
    "### Pros of the Evaluation Metric:\n",
    "It is an easy to calculate evaluation metric.\n",
    "All the errors are weighted on the same scale since absolute values are taken.\n",
    "It is useful if the training data has outliers as MAE does not penalize high errors caused by outliers.\n",
    "It provides an even measure of how well the model is performing.\n",
    "### Cons of the evaluation metric:\n",
    "Sometimes the large errors coming from the outliers end up being treated as the same as low errors.\n",
    "MAE follows a scale-dependent accuracy measure where it uses the same scale as the data being measured. Hence it cannot be used to compare series’ using different measures.\n",
    "One of the main disadvantages of MAE is that it is not differentiable at zero. Many optimization algorithms tend to use differentiation to find the optimum value for parameters in the evaluation metric.\n",
    "It can be challenging to compute gradients in MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d776844-a85b-4410-9b58-a3cc0917a652",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd219ae-2517-49cd-ae5d-6cb9af2e379c",
   "metadata": {},
   "source": [
    "### Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss.\n",
    "### The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7fd7e-0a94-4ac1-b07d-d4b295e7deff",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86204c-4f88-48a7-a646-a0f4bb976763",
   "metadata": {},
   "source": [
    "### Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation. Thus, if the coefficient inflates, the cost function will increase. And Linear regression model will try to optimize the coefficient in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be6309-1965-43d3-b312-ffdfec409223",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e0031-d284-47ed-ad3f-c7ee4e836e6a",
   "metadata": {},
   "source": [
    "### Limitations:\n",
    "1. They include all the predictors in the final model.\n",
    "2. They are unable to perform feature selection.\n",
    "3. They shrink the coefficients towards zero.\n",
    "4. They trade the variance for bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe778d-5ed6-4130-8d91-2b8f0059d9e2",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2fd5c-4159-493f-8728-a3565eb2f18e",
   "metadata": {},
   "source": [
    "### I will choose Model B because its MAE value is less and MAE is robust to outliers.\n",
    "### Limitations of MAE is that it is time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fdb9e-06d1-4060-965b-43c8eea521b4",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a3c4d-90ea-46dc-a443-65abd8fbc1b4",
   "metadata": {},
   "source": [
    "### I will choose the Model B because its regularization value is high and it will help in both feature selection and reducing the overfitting.\n",
    "###  Limitation Is That If There Are Two Or More Highly Collinear Variables Then Lasso Regression Will Select One Of Them Randomly Which Is Not A Good Technique In Data Interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f16172-d8b2-426c-a3ee-b5c63e6ed016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
